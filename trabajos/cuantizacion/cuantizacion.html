<!DOCTYPE html>
<html lang="es">
<head>
    <!--Elaborado por Julio Luengo Mendoza-->
    <meta charset="UTF-8">
    <title>Cuantización Modelos IA | Portfolio Julio LM</title>
    <style>
        body {
            background: #222129;
            color: #f0bd8b;
            font-family: 'Courier New', Courier, monospace;
            margin: 0;
            padding: 5%;
        }
        header, main {
            max-width: 960px;
            margin: auto;
            padding: 32px 15px 0 15px;
        }
        .titulo-bloque {
            display: flex;
            align-items: center;
            margin-top: 32px;
        }
        .tag {
            background: #f0bd8b;
            color: #222129;
            padding: 8px 18px;
            font-weight: bold;
            margin-right: 12px;
            border-radius: 2px;
            letter-spacing: 0.5px;
        }
        .barra {
            flex: 1;
            height: 12px;
            background: repeating-linear-gradient(
                to right,
                #f0bd8b,
                #f0bd8b 2px,
                transparent 2px,
                transparent 8px
            );
        }
        h1 {
            margin-top: 28px;
            background: #1d1c23;
            padding: 36px 0;
            font-size: 2.1em;
            letter-spacing: 2px;
            border: 2px solid #f0bd8b;
            text-align: center;
        }
        h2, h3 {
            color: #f0bd8b;
            margin-top: 36px;
        }
        .fecha-autor {
            color: #c8a979;
            font-size: 1em;
            margin-bottom: 10px;
            text-align: right;
        }
        .autor-block {
            text-align: center;
            margin: 10px 0 42px 0;
        }
        .indice {
            margin: 32px 0 40px 0;
            padding: 24px;
            background: #1d1c23;
            border-left: 3px solid #f0bd8b;
            border-radius: 7px;
        }
        .indice a {
            display: block;
            color: #ffc686;
            margin-bottom: 9px;
            font-size: 1.12em;
            text-decoration: none;
        }
        .indice a:hover {
            text-decoration: underline;
        }
        section {
            border-top: 2px dotted #f0bd8b;
            margin-top: 30px;
            padding-top: 16px;
        }
        img {
            width: 100%;
            max-width: 550px;
            margin: 16px 0 5px 0;
            border: 2px solid #f0bd8b;
            background: #191821;
            box-shadow: 0 2px 18px rgba(100,90,50,0.08);
            display: block;
        }
        .pie-foto {
            color: #c8a979;
            font-size: 0.95em;
            margin-bottom: 14px;
            text-align: center;
        }
        nav {
            margin-bottom: 24px;
            text-align: left;
        }
        a.volver {
            color: #ffc686;
            text-decoration: none;
            font-size: 1.08em;
        }
        a.volver:hover {
            text-decoration: underline;
        }
        ul, ol {
            margin-left: 24px;
        }
        hr {
            border: 1px solid #f0bd8b;
            margin: 38px 0 28px 0;
        }
        .content-block {
            background: #24232a;
            border-radius: 6px;
            padding: 18px 26px;
            margin: 25px 0 0 0;
            box-shadow: 0 2px 18px rgba(110,100,56,0.07);
        }
        .code {
            background: #292832;
            color: #f0bd8b;
            padding: 6px 14px;
            border-radius: 4px;
            width: fit-content;
            font-size: 1.03em;
            font-family: inherit;
            margin: 8px 0;
            display: inline-block;
        }
        pre {
            background: #292832;
            color: #f0bd8b;
            padding: 16px;
            border-radius: 6px;
            overflow-x: auto;
            border-left: 3px solid #f0bd8b;
            margin: 16px 0;
            font-size: 0.95em;
            line-height: 1.5;
        }
        .codigo {
            white-space: pre-wrap;
            background: #292832;
            border-left: 3px solid #f0bd8b;
            padding: 16px;
            margin: 16px 0;
            border-radius: 6px;
            font-size: 0.92em;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 16px 0;
        }
        th, td {
            border: 1px solid #f0bd8b;
            padding: 8px 12px;
            text-align: left;
        }
        th {
            background: #292832;
        }
        a {
            color: #ffc686;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        .nota {
            background: #1d1c23;
            border: 1px solid #f0bd8b;
            padding: 16px;
            border-radius: 6px;
            margin: 16px 0;
        }
    </style>
</head>
<body>
    <header>
        <div class="titulo-bloque">
            <span class="tag"><a href="../../index.html">IA AVANZADA</a></span>
            <div class="barra"></div>
        </div>
        <h1>Cuantización Phi-3 Mini</h1>
    </header>
    
    <nav>
        <a href="../../index.html" class="volver">← Volver al índice</a>
    </nav>
    
    <div class="autor-block">Elaborado por: Julio Luengo Mendoza</div>
    
    <div class="indice">
        <a href="#objetivo">OBJETIVO</a>
        <a href="#entorno">PREPARAR ENTORNO</a>
        <a href="#modelo">DESCARGAR MODELO</a>
        <a href="#conversion">CONVERSIÓN GGUF</a>
        <a href="#cuantizacion">CUANTIZACIÓN Q4</a>
        <a href="#tamano">REDUCCIÓN TAMAÑO</a>
        <a href="#uso">EJECUTAR CUANTIZADO</a>
    </div>

    <section id="objetivo">
        <h2>OBJETIVO</h2>
        <div class="content-block">
            Convertir modelo <strong>Phi-3-mini-4k-instruct</strong> (Microsoft, FP16 ~8GB) → <strong>GGUF Q4_K_M</strong> (~2.3GB).<br><br>
            Reducción 70% tamaño + ejecución CPU/GPU eficiente. Herramienta: <strong>llama.cpp</strong> oficial.
        </div>
    </section>

    <section id="entorno">
        <h2>PREPARAR ENTORNO</h2>
        <div class="content-block">
            <ol>
                <li>Crear directorio proyecto:
                    <pre><code>mkdir ejercicio-cuantizacion
cd ejercicio-cuantizacion</code></pre>
                </li>
                <li>Clonar llama.cpp (133MB):
                    <img src="img/Captura1.png" alt="Clonando llama.cpp PowerShell">
                    <div class="pie-foto">Figura 1: Clonación exitosa llama.cpp (133MB, 77k objetos).</div>
                </li>
                <li>Entorno virtual + dependencias:
                    <img src="img/Captura2.png" alt="Instalación requirements pip">
                    <div class="pie-foto">Figura 2: pip install requirements-convert-hf-to-gguf.txt (transformers, gguf, torch CPU).</div>
                    <pre><code>python -m venv venv
venv\Scripts\activate  # Windows
pip install -r requirements/requirements-convert-hf-to-gguf.txt</code></pre>
                </li>
            </ol>
            <div class="nota">Dependencias: numpy 1.26.4, transformers 4.45+, gguf 0.17+, torch CPU.</div>
        </div>
    </section>

    <section id="modelo">
        <h2>DESCARGAR MODELO ORIGINAL</h2>
        <div class="content-block">
            <pre><code>git clone https://huggingface.co/microsoft/Phi-3-mini-4k-instruct</code></pre>
            <img src="img/Captura3.png" alt="Descarga Phi-3 HuggingFace git clone">
            <div class="pie-foto">Figura 3: Descarga Phi-3-mini-4k-instruct (~8GB, 100% completado).</div>
            <ul>
                <li>Tamaño original: ~8GB (FP16, archivos .safetensors)</li>
                <li>Contenido: config.json, tokenizer.json, weights</li>
                <li>Requiere <span class="code">git-lfs</span> instalado</li>
            </ul>
        </div>
    </section>

    <section id="conversion">
        <h2>CONVERSIÓN HF → GGUF</h2>
        <div class="content-block">
            Convertir HuggingFace → formato GGUF unificado:
            <pre><code>python convert-hf-to-gguf.py ../Phi-3-mini-4k-instruct --outtype f16 --outfile phi-3-mini-f16.gguf</code></pre>
            <img src="img/Captura4.png" alt="Ejecutando convert-hf-to-gguf.py">
            <div class="pie-foto">Figura 4: Conversión HF→F16 GGUF en progreso.</div>
            <ul>
                <li><span class="code">--outtype f16</span>: Precisión original</li>
                <li>Tamaño: ~7.8GB (archivo único .gguf)</li>
                <li>Tiempo: 5-15min</li>
            </ul>
        </div>
    </section>

    <section id="cuantizacion">
        <h2>CUANTIZACIÓN Q4_K_M</h2>
        <div class="content-block">
            <pre><code>./llama-quantize phi-3-mini-f16.gguf phi-3-mini-q4.gguf Q4_K_M</code></pre>
            <img src="img/Captura5.png" alt="Ejecutando llama-quantize Q4_K_M">
            <div class="pie-foto">Figura 5: Cuantización F16→Q4_K_M activa.</div>
            <ul>
                <li><span class="code">Q4_K_M</span>: 4bits optimizado bloques K</li>
                <li>Tiempo: 10-20min</li>
                <li>Requiere compilar: <span class="code">make</span></li>
            </ul>
        </div>
    </section>

    <section id="tamano">
        <h2>REDUCCIÓN TAMAÑO - RESULTADOS</h2>
        <div class="content-block">
            <img src="img/Captura6.png" alt="LM Studio mostrando modelos cuantizados">
            <div class="pie-foto">Figura 6: LM Studio - phi-3-mini.gguf (7.4MB) vs phi-3-mini-q4.gguf (1.9MB).</div>
            
            <table>
                <tr>
                    <th>Formato</th>
                    <th>Tamaño</th>
                    <th>Precisión</th>
                    <th>Velocidad</th>
                </tr>
                <tr>
                    <td>FP16 Original</td>
                    <td>8.0 GB</td>
                    <td>100%</td>
                    <td>Lenta</td>
                </tr>
                <tr>
                    <td>GGUF F16</td>
                    <td>7.8 GB</td>
                    <td>100%</td>
                    <td>Media</td>
                </tr>
                <tr>
                    <td>Q4_K_M</td>
                    <td>1.9 GB</td>
                    <td>~98%</td>
                    <td>Rápida</td>
                </tr>
            </table>
        </div>
    </section>

    <section id="uso">
        <h2>EJECUTAR + VERIFICAR</h2>
        <div class="content-block">
            Inferencia cuantizado:
            <pre><code>./llama-cli -m phi-3-mini-q4.gguf -p "Explica cuantización" -n 128</code></pre>
            
            <img src="img/Captura7.png" alt="PowerShell ejecutando llama-cli modelo cuantizado">
            <div class="pie-foto">Figura 7: Inferencia exitosa phi-3-mini-q4.gguf vía llama-cli.</div>
            
            <img src="img/Captura8.png" alt="LM Studio chat con modelo cuantizado">
            <div class="pie-foto">Figura 8: LM Studio probando phi-3-mini-q4.gguf (respuesta coherente).</div>
            
            <ul>
                <li>Velocidad CPU: 20-40 tokens/s</li>
                <li>Compatible: Ollama, LM Studio, llama.cpp</li>
            </ul>
        </div>
    </section>

    <nav style="margin-top: 48px;">
        <a href="../../index.html" class="volver">← Volver al índice IA</a>
    </nav>
</body>
</html>
